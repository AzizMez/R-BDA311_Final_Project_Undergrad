---
title: "Latent Dirichlet Allocation "
author: "Aziz Al Mezraani"
date: "9/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lemon)
knit_print.data.frame <- lemon_print
```

## Bringing in the Data and Cleaning it

Begin by importing the data:
```{r data import}
#set your working directory to the folder where you have the data and the scrip
#Now read in the data
ShortAnswerQs<-read.csv("SurveyRawData.csv",stringsAsFactors = FALSE)
```

### Step 1: Text Extraction and Creating a Corpus

In text analysis a corpus (a latin word meaning body) is a collection of text documents which you are interested in analyzing. In our data the text appears in the first three columns, which relate to the three open-ended questions that were asked about potential opportunities, programs, and whether the presence should be virtual or real. Thus we need to render each entry from each variable vector into a ``document'' within the corpus. Doing this requires the "tm" library.

```{r make corpus}
# Load the 'tm' package for text mining
library(tm)

# Create a text corpus for each short answer question
# Each corpus will be processed separately for topic modeling
answerCorpus_Q1 <- Corpus(VectorSource(ShortAnswerQs$Q1_Opps))
answerCorpus_Q2 <- Corpus(VectorSource(ShortAnswerQs$Q2_Progs))
answerCorpus_Q3 <- Corpus(VectorSource(ShortAnswerQs$Q3_Presence))
```

### Step 2: Text Pre-Processing

In order to get a meaningful view into the text, we will need to remove those things that make sense to humans, but not so much sense to machines. Specifically, we will:

* Remove capitalization
* Remove "stop-words" -- words like a, an, the, he, she, etc.
* Remove punctuation

These steps depend on the "tm" and "SnowballC" packages.

```{r pre-processing}
library(SnowballC)

#Remove capital letters; convert them to lower case
answerCorpus_Q1<-tm_map(answerCorpus_Q1, tolower)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, tolower)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, tolower)
```

```{r remove stopwords}
#Remove standard stopwords
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removeWords, stopwords("english"))
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removeWords, stopwords("english"))
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removeWords, stopwords("english"))

#Remove context specific words; feel free to add some of your own!
contextWords<-c("school","university","started","can","struggle", "need","also","because","music")
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removeWords, contextWords)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removeWords, contextWords)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removeWords, contextWords)

#Remove punctuation
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removePunctuation)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removePunctuation)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removePunctuation)
```

### Step 3: Creating the document-term matrix and the term-document matrix

The document-term matrix (dtm) is a matrix where each row is a review and each column (ie variable) is a word, the entries of the matrix indicate the prevalence of the word in the document. In contrast the term-document matrix (tdm) is the transpose of the dtm -- that is the rows are words and the columns are documents. Each has a different value in text analysis and thus it will be handy to create both.

```{r create dtm and tdm}
# Create Document-Term Matrices for each question
answerQ1_dtm <- DocumentTermMatrix(answerCorpus_Q1)
answerQ2_dtm <- DocumentTermMatrix(answerCorpus_Q2)
answerQ3_dtm <- DocumentTermMatrix(answerCorpus_Q3)

# Create Term-Document Matrices for flexibility in visualization or analysis
answerQ1_tdm <- TermDocumentMatrix(answerCorpus_Q1)
answerQ2_tdm <- TermDocumentMatrix(answerCorpus_Q2)
answerQ3_tdm <- TermDocumentMatrix(answerCorpus_Q3)
````
## Latent Dirichlet Allocation

This technique allows us to build our words into topics. In Latent Dirichlet Allocation (LDA), we will need to select the number of topics that are appropriate to the corpus we have (this should remind you of something...kMeans clustering, perhaps?). Once we choose the number of topics, the LDA algorithm will allocate words to topics in a way that reflects the words' relationship to the topic. If you have too many topics some of the word relationships will be weak; if you have too few topics you will lose meaning. One way to obtain the right number of topics is through parameter tuning, which requires building a new model for each choice of the number of topics. Here, we will just select a number that seems reasonable and see what it tells us. This will require the "topicmodels" package.

```{r LDA topics, render=lemon_print}
library(topicmodels)
#Sum up the number of words in each document and then create a vector of these sums as long as they are non-zero
row.sum<-apply(answerQ1_dtm,1,FUN=sum)
answerQ1_dtm <- answerQ1_dtm[row.sum!=0,]

row.sum<-apply(answerQ2_dtm,1,FUN=sum)
answerQ2_dtm <- answerQ2_dtm[row.sum!=0,]

row.sum<-apply(answerQ3_dtm,1,FUN=sum)
answerQ3_dtm <- answerQ3_dtm[row.sum!=0,]

#3 topics is a good size for this dataset
# Fit 3-topic LDA models to each question's cleaned document-term matrix

answersQ1_3Topics_lda<-LDA(answerQ1_dtm, k = 3)
as.data.frame(get_terms(answersQ1_3Topics_lda, 10))

answersQ2_3Topics_lda<-LDA(answerQ2_dtm, k = 3)
as.data.frame(get_terms(answersQ2_3Topics_lda, 10))

answersQ3_3Topics_lda<-LDA(answerQ3_dtm, k = 3)
as.data.frame(get_terms(answersQ3_3Topics_lda, 10))

```

Now we can visualize what the word probabilities within each topic look like. We will use the "wordcloud" package for this.

```{r LDA visualization}
# Load tidy tools and plotting libraries for visualizing LDA results
library(dplyr)
library(tidytext)
library(data.table)
library(RColorBrewer)
library(wordcloud)

# Define a nice palette for topic word clouds
dark2 <- brewer.pal(8, "Dark2") #creates a nice color palette
```

### Question 1

```{r Question 1 LDA Viz}
# Tidy Q1 model: extract word-topic probabilities
answersQ1_3topics <- tidy(answersQ1_3Topics_lda, matrix = "beta")

# Get top 30 terms per topic
top_termsQ1 <- answersQ1_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Reshape for comparison cloud
termsQ1<-dcast(as.data.table(top_termsQ1), term ~ topic, value.var = c("beta"))
names<-termsQ1$term
termsQ1<-termsQ1[,-c(1)]
colnames(termsQ1)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ1[is.na(termsQ1)]<-0
termsQ1<-as.matrix(termsQ1)
rownames(termsQ1)<-names

# Plot word cloud comparing top terms across topics
comparison.cloud(termsQ1,max.words=30,random.order=FALSE,scale=c(3,.1),title.size=1.4)

```

### Question 2

```{r Question 2 LDA Viz}
# Repeat LDA visualization for Q2
answersQ2_3topics <- tidy(answersQ2_3Topics_lda, matrix = "beta")

top_termsQ2 <- answersQ2_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

termsQ2<-dcast(as.data.table(top_termsQ2), term ~ topic, value.var = c("beta"))
names<-termsQ2$term
termsQ2<-termsQ2[,-c(1)]
colnames(termsQ2)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ2[is.na(termsQ2)]<-0
termsQ2<-as.matrix(termsQ2)
rownames(termsQ2)<-names

# Generate word cloud for Q2 topics
comparison.cloud(termsQ2,max.words=30,random.order=FALSE,scale=c(3,.1),,title.size=1.4)

```

### Question 3

```{r Question 3 LDA Viz}
# Repeat LDA visualization for Q3
answersQ3_3topics <- tidy(answersQ3_3Topics_lda, matrix = "beta")

top_termsQ3 <- answersQ3_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

termsQ3<-dcast(as.data.table(top_termsQ3), term ~ topic, value.var = c("beta"))
names<-termsQ3$term
termsQ3<-termsQ3[,-c(1)]
colnames(termsQ3)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ3[is.na(termsQ3)]<-0
termsQ3<-as.matrix(termsQ3)
rownames(termsQ3)<-names

# Generate word cloud for Q3 topics
comparison.cloud(termsQ3,max.words=30,random.order=FALSE,scale=c(3,.1),,title.size=1.4)

```
```
