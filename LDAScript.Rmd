---
title: "Latent Dirichlet Allocation Tutorial"
author: "F. Jordan Srour"
date: "9/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lemon)
knit_print.data.frame <- lemon_print
```

## Bringing in the Data and Cleaning it

Begin by importing the data:
```{r data import}
#set your working directory to the folder where you have the data and the scrip
#Now read in the data
ShortAnswerQs<-read.csv("SurveyRawData.csv",stringsAsFactors = FALSE)
```

### Step 1: Text Extraction and Creating a Corpus

In text analysis a corpus (a latin word meaning body) is a collection of text documents which you are interested in analyzing. In our data the text appears in the first three columns, which relate to the three open-ended questions that were asked about potential opportunities, programs, and whether the presence should be virtual or real. Thus we need to render each entry from each variable vector into a ``document'' within the corpus. Doing this requires the "tm" library.

```{r make corpus}
library(tm)

answerCorpus_Q1<-Corpus(VectorSource(ShortAnswerQs$Q1_Opps))
answerCorpus_Q2<-Corpus(VectorSource(ShortAnswerQs$Q2_Progs))
answerCorpus_Q3<-Corpus(VectorSource(ShortAnswerQs$Q3_Presence))
```

### Step 2: Text Pre-Processing

In order to get a meaningful view into the text, we will need to remove those things that make sense to humans, but not so much sense to machines. Specifically, we will:

* Remove capitalization
* Remove "stop-words" -- words like a, an, the, he, she, etc.
* Remove punctuation

These steps depend on the "tm" and "SnowballC" packages.

```{r pre-processing}
library(SnowballC)

#Remove capital letters; convert them to lower case
answerCorpus_Q1<-tm_map(answerCorpus_Q1, tolower)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, tolower)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, tolower)
```

```{r remove stopwords}
#Remove standard stopwords
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removeWords, stopwords("english"))
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removeWords, stopwords("english"))
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removeWords, stopwords("english"))

#Remove context specific words; feel free to add some of your own!
contextWords<-c("school","university","started","can","struggle", "need","also","because","music")
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removeWords, contextWords)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removeWords, contextWords)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removeWords, contextWords)

#Remove punctuation
answerCorpus_Q1<-tm_map(answerCorpus_Q1, removePunctuation)
answerCorpus_Q2<-tm_map(answerCorpus_Q2, removePunctuation)
answerCorpus_Q3<-tm_map(answerCorpus_Q3, removePunctuation)
```

### Step 3: Creating the document-term matrix and the term-document matrix

The document-term matrix (dtm) is a matrix where each row is a review and each column (ie variable) is a word, the entries of the matrix indicate the prevalence of the word in the document. In contrast the term-document matrix (tdm) is the transpose of the dtm -- that is the rows are words and the columns are documents. Each has a different value in text analysis and thus it will be handy to create both.

```{r create dtm and tdm}
answerQ1_dtm<-DocumentTermMatrix(answerCorpus_Q1)
answerQ2_dtm<-DocumentTermMatrix(answerCorpus_Q2)
answerQ3_dtm<-DocumentTermMatrix(answerCorpus_Q3)

answerQ1_tdm<-TermDocumentMatrix(answerCorpus_Q1)
answerQ2_tdm<-TermDocumentMatrix(answerCorpus_Q2)
answerQ3_tdm<-TermDocumentMatrix(answerCorpus_Q3)
````
## Latent Dirichlet Allocation

This technique allows us to build our words into topics. In Latent Dirichlet Allocation (LDA), we will need to select the number of topics that are appropriate to the corpus we have (this should remind you of something...kMeans clustering, perhaps?). Once we choose the number of topics, the LDA algorithm will allocate words to topics in a way that reflects the words' relationship to the topic. If you have too many topics some of the word relationships will be weak; if you have too few topics you will lose meaning. One way to obtain the right number of topics is through parameter tuning, which requires building a new model for each choice of the number of topics. Here, we will just select a number that seems reasonable and see what it tells us. This will require the "topicmodels" package.

```{r LDA topics, render=lemon_print}
library(topicmodels)
#Sum up the number of words in each document and then create a vector of these sums as long as they are non-zero
row.sum<-apply(answerQ1_dtm,1,FUN=sum)
answerQ1_dtm <- answerQ1_dtm[row.sum!=0,]

row.sum<-apply(answerQ2_dtm,1,FUN=sum)
answerQ2_dtm <- answerQ2_dtm[row.sum!=0,]

row.sum<-apply(answerQ3_dtm,1,FUN=sum)
answerQ3_dtm <- answerQ3_dtm[row.sum!=0,]

#3 topics is a good size for this dataset

answersQ1_3Topics_lda<-LDA(answerQ1_dtm, k = 3)
as.data.frame(get_terms(answersQ1_3Topics_lda, 10))

answersQ2_3Topics_lda<-LDA(answerQ2_dtm, k = 3)
as.data.frame(get_terms(answersQ2_3Topics_lda, 10))

answersQ3_3Topics_lda<-LDA(answerQ3_dtm, k = 3)
as.data.frame(get_terms(answersQ3_3Topics_lda, 10))

```

Now we can visualize what the word probabilities within each topic look like. We will use the "wordcloud" package for this.

```{r LDA visualization}
library(dplyr)
library(tidytext)
library(data.table)
library(RColorBrewer)
library(wordcloud)
dark2 <- brewer.pal(8, "Dark2") #creates a nice color palette
```

### Question 1

```{r Question 1 LDA Viz}
answersQ1_3topics <- tidy(answersQ1_3Topics_lda, matrix = "beta")

top_termsQ1 <- answersQ1_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

termsQ1<-dcast(as.data.table(top_termsQ1), term ~ topic, value.var = c("beta"))
names<-termsQ1$term
termsQ1<-termsQ1[,-c(1)]
colnames(termsQ1)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ1[is.na(termsQ1)]<-0
termsQ1<-as.matrix(termsQ1)
rownames(termsQ1)<-names

comparison.cloud(termsQ1,max.words=30,random.order=FALSE,scale=c(3,.1),title.size=1.4)

```

### Question 2

```{r Question 2 LDA Viz}
answersQ2_3topics <- tidy(answersQ2_3Topics_lda, matrix = "beta")

top_termsQ2 <- answersQ2_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

termsQ2<-dcast(as.data.table(top_termsQ2), term ~ topic, value.var = c("beta"))
names<-termsQ2$term
termsQ2<-termsQ2[,-c(1)]
colnames(termsQ2)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ2[is.na(termsQ2)]<-0
termsQ2<-as.matrix(termsQ2)
rownames(termsQ2)<-names

comparison.cloud(termsQ2,max.words=30,random.order=FALSE,scale=c(3,.1),,title.size=1.4)

```

### Question 3

```{r Question 3 LDA Viz}
answersQ3_3topics <- tidy(answersQ3_3Topics_lda, matrix = "beta")

top_termsQ3 <- answersQ3_3topics %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

termsQ3<-dcast(as.data.table(top_termsQ3), term ~ topic, value.var = c("beta"))
names<-termsQ3$term
termsQ3<-termsQ3[,-c(1)]
colnames(termsQ3)<-c("Topic 1", "Topic 2", "Topic 3")
termsQ3[is.na(termsQ3)]<-0
termsQ3<-as.matrix(termsQ3)
rownames(termsQ3)<-names

comparison.cloud(termsQ3,max.words=30,random.order=FALSE,scale=c(3,.1),,title.size=1.4)

```
```
